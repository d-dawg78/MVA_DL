{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1Z-D2zvcMI6"
   },
   "outputs": [],
   "source": [
    "# librairies\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import string \n",
    "import sys\n",
    "import os \n",
    "\n",
    "# for visualization \n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8dzPA4UcMJE",
    "outputId": "49d46169-f8ea-4dbf-d556-ad32bece5e06"
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install keras\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "import keras\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Bidirectional, SpatialDropout1D, SimpleRNN\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, MaxPooling2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78tPish3gU4w",
    "outputId": "b6c841d9-d59e-4024-ddc7-780f43291bae"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8KUz2g0f1HB",
    "outputId": "64c72111-649d-4633-8ffc-b4eec6c13a28"
   },
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CVSUXBTcMJF"
   },
   "source": [
    "## Processing the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGqE4RDKcMJP"
   },
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "\n",
    "# Path Kodjo\n",
    "# path = '/home/anselme/Desktop/Etude/MVA_S1/DL/MVA_DL/Data Mining'\n",
    "\n",
    "dataset = pd.read_excel('dataset.xlsx', skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "HYp-yhfqcMJQ",
    "outputId": "43cb303a-850a-47d6-f0d1-d008db88b755"
   },
   "outputs": [],
   "source": [
    "dataset['Class'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "j1qtTjnvpBAx",
    "outputId": "a730a4c8-da84-4b8b-cced-c53ea9aa7ed1"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "dataset = shuffle(dataset)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJYGZKoLcMJS",
    "outputId": "853ac1f2-474d-4e8b-affb-4756abe9c543"
   },
   "outputs": [],
   "source": [
    "# Process lyrics\n",
    "dataset[\"Lyrics\"] = dataset[\"Lyrics\"].str.replace('chorus','')\n",
    "\n",
    "# Lyrics\n",
    "Lyrics = [dataset['Lyrics'][i] for i in range(dataset.shape[0])]\n",
    "\n",
    "# labels \n",
    "yData  = [dataset['Class'][i]  for i in range(dataset.shape[0])]\n",
    "\n",
    "\n",
    "# remove text between [] (corresponds to Chrous / verse headings)\n",
    "Lyrics = [re.sub(\"\\[.*?\\]\", \"\",Lyrics[i]) for i in range( dataset.shape[0])]\n",
    "\n",
    "# Clean up data (replace \\n, [, (, ], ) par \"\")\n",
    "Lyrics = [Lyrics[i].replace('\\n', ' ') for i in  range( dataset.shape[0])]\n",
    "Lyrics = [Lyrics[i].replace('[' ,  '') for i in  range( dataset.shape[0])]\n",
    "Lyrics = [Lyrics[i].replace(']' ,  '') for i in  range( dataset.shape[0])]\n",
    "Lyrics = [Lyrics[i].replace('(' ,  '') for i in  range( dataset.shape[0])]\n",
    "Lyrics = [Lyrics[i].replace(')' ,  '') for i in  range( dataset.shape[0])]\n",
    "\n",
    "#for i in range(100):\n",
    "#  print(Lyrics[i])\n",
    "\n",
    "xData = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "for Lyric in Lyrics:\n",
    "    # Tokenize each lyric, and set all characters to lower-case\n",
    "    tokens = word_tokenize(Lyric)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens_nopunc = [word.translate(translator) for word in tokens]\n",
    "    \n",
    "    # Remove non-alphabetic tokens\n",
    "    words = [word for word in tokens_nopunc if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words from the lyric\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    \n",
    "    # Append to training data\n",
    "    xData.append(words)\n",
    "\n",
    "for i in range(10):\n",
    "  print(xData[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neDiWO8TcMJT"
   },
   "outputs": [],
   "source": [
    "# Loads pre-trained GloVe word embeddings\n",
    "\n",
    "# Load in GloVe file and initialize embedding index\n",
    "filename = 'wiki-news-300d-1M.vec'\n",
    "file = open(os.path.join('', filename), encoding = \"utf-8\")\n",
    "embeddings_index = {}\n",
    "\n",
    "for line in file:\n",
    "\n",
    "    # Add each embedding to the embedding index\n",
    "    embedding = line.split()\n",
    "    embeddings_index[embedding[0]] = np.asarray(embedding[1:])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lT68I2MRcMJT"
   },
   "outputs": [],
   "source": [
    "# Map each word token in the training data to an integer\n",
    "\n",
    "# For each training example, maps each word token to an integer\n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(xData)\n",
    "xData_seq = tokenizer.texts_to_sequences(xData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZ--53z5cMJU"
   },
   "outputs": [],
   "source": [
    "# Pad sequences shorter than max length \n",
    "max_num_tokens = max([len(tokenized_lyric) for tokenized_lyric in xData])\n",
    "print(max_num_tokens)\n",
    "xData_seq_padded = pad_sequences(xData_seq, maxlen=max_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXroeg8YcMJU"
   },
   "outputs": [],
   "source": [
    "# Map GloVe word embeddings to each word in the tokenizer word index to create a matrix of word embeddings\n",
    "\n",
    "# Initialize embedding matrix\n",
    "EMBEDDING_DIM = 300\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "\n",
    "print(num_words)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "num = 0\n",
    "# Populate embedding matrix\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    if i > num_words:\n",
    "        #print(word)\n",
    "        num = num + 1\n",
    "        continue\n",
    "        \n",
    "    # Assign corresponding GloVe embedding to the given word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding are assigned a zero vector by default\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        if (all(abs(float(v)) < 0.0000000001 for v in embedding_vector)):\n",
    "            #print(word)\n",
    "            num = num + 1\n",
    "            \n",
    "    else:\n",
    "        #print(word)\n",
    "        num = num + 1\n",
    "        \n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in xData for item in sublist]\n",
    "\n",
    "print(len(flat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HS5MM-7ocMJV",
    "outputId": "fb9cceb0-c223-47ca-a7f1-5625be03cf5e"
   },
   "outputs": [],
   "source": [
    "# Shuffles the data, and splits it into train and test sets\n",
    "np.random.seed(0)\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "word_indices = np.arange(xData_seq_padded.shape[0])\n",
    "np.random.shuffle(word_indices)\n",
    "xData_seq_padded = xData_seq_padded[word_indices]\n",
    "mood = np.array(yData)\n",
    "mood = mood[word_indices]\n",
    "\n",
    "# Binarizes the mood labels\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "moods = encoder.fit_transform(mood.tolist())\n",
    "\n",
    "# Splits the dataset into train and test sets\n",
    "num_validation_samples = int(VALIDATION_SPLIT * xData_seq_padded.shape[0])\n",
    "\n",
    "X_train_pad = xData_seq_padded[:-(2 * num_validation_samples)]\n",
    "y_train = moods[:-(2 * num_validation_samples)]\n",
    "\n",
    "X_val_pad = xData_seq_padded[-(2 * num_validation_samples):-num_validation_samples]\n",
    "y_val = moods[-(2 * num_validation_samples):-num_validation_samples]\n",
    "\n",
    "X_test_pad = xData_seq_padded[-num_validation_samples:]\n",
    "y_test = moods[-num_validation_samples:]\n",
    "\n",
    "print(\"Train set length: {}\".format(len(X_train_pad)))\n",
    "print(\"Validation set length: {}\".format(len(X_val_pad)))\n",
    "print(\"Test set length: {}\".format(len(X_test_pad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HvQ9RfyvwSj",
    "outputId": "9f9e0870-5e41-440b-f702-d05d7cb0843c"
   },
   "outputs": [],
   "source": [
    "# Defines the Multiclass model, compiles and trains it\n",
    "from keras.layers import add\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words, \n",
    "                            EMBEDDING_DIM, \n",
    "                            embeddings_initializer=Constant(embedding_matrix), \n",
    "                            input_length=max_num_tokens,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Conv1D(256, 3, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "checkpoint_filepath = 'checkpoint1'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1)\n",
    "\n",
    "op = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model.compile(optimizer = op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Saves a History object for plotting\n",
    "history = model.fit(X_train_pad, y_train, batch_size=16, epochs=40, \n",
    "                    validation_data=(X_val_pad, y_val), verbose=2, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "64gykjkMcMJX",
    "outputId": "6ae465c6-cc5f-4d1b-cf3b-0226d2c6020b"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy as a function of training epoch\n",
    "\n",
    "figure(num=None, figsize=(5, 5), dpi=300)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.grid(True)\n",
    "plt.title('CNN with Fasttext Embeddings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoIygwLjfSgc",
    "outputId": "08aebc2c-a8e8-4970-b3cd-296b7799e97e"
   },
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "results = model.evaluate(X_test_pad, y_test, batch_size=16)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bwTw32tjcMJY",
    "outputId": "c60128ba-9846-4349-a5ff-c84e9827c3dd"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "alphabetical = []\n",
    "\n",
    "for el in mood:\n",
    "    if (el not in alphabetical):\n",
    "        alphabetical.append(el)\n",
    "\n",
    "alphabetical = sorted(alphabetical)\n",
    "\n",
    "# Gets predicted labels from model\n",
    "y_pred = model.predict(X_test_pad)\n",
    "\n",
    "# Formats labels into appropriate class designations\n",
    "y_pred_classes = np.zeros(len(y_pred)).astype(int)\n",
    "y_test_classes = np.zeros(len(y_test)).astype(int)\n",
    "\n",
    "for index, label in enumerate(y_pred):\n",
    "    y_pred_classes[index] = np.argmax(y_pred[index])\n",
    "    \n",
    "for index, label in enumerate(y_test):\n",
    "    y_test_classes[index] = np.argmax(y_test[index])\n",
    "\n",
    "# Generates confusion matrix\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "# Formats and displays the confusion matrix\n",
    "figure(num=None, figsize=(5, 5), dpi=300)\n",
    "df_cm = pd.DataFrame(cm, index = alphabetical, columns = alphabetical)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=plt.cm.Blues, fmt='g', cbar=False)\n",
    "plt.title('CNN with Fasttext Embeddings Confusion Matrix', fontsize=9)\n",
    "plt.xlabel(\"Predicted Class\", fontsize=9)\n",
    "plt.ylabel(\"Actual Class\", fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hH60_6MO4im_",
    "outputId": "6ed54280-58ff-49ea-8a1f-b213b9020db3"
   },
   "outputs": [],
   "source": [
    "# Cell for getting information about each class\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "new_test_classes = np.full(shape=y_test_classes.shape[0], fill_value=\"yoooooooooooooooo\")\n",
    "new_pred_classes = np.full(shape=y_pred_classes.shape[0], fill_value=\"yoooooooooooooooo\")\n",
    "\n",
    "for x in range(len(y_test_classes)):\n",
    "    id1 = y_test_classes[x]\n",
    "    id2 = y_pred_classes[x]\n",
    "\n",
    "    word1 = alphabetical[id1]\n",
    "    word2 = alphabetical[id2]\n",
    "\n",
    "    new_test_classes[x] = word1\n",
    "    new_pred_classes[x] = word2\n",
    "\n",
    "print(classification_report(new_test_classes, new_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERfxIp7Jzy4w",
    "outputId": "cbd009b4-2da6-4c4d-a82f-51237618d434"
   },
   "outputs": [],
   "source": [
    "# Defines the Multiclass model, compiles and trains it\n",
    "\n",
    "lstm = Sequential()\n",
    "embedding_layer = Embedding(num_words, \n",
    "                            EMBEDDING_DIM, \n",
    "                            embeddings_initializer=Constant(embedding_matrix), \n",
    "                            input_length=max_num_tokens,\n",
    "                            trainable=False)\n",
    "\n",
    "lstm.add(embedding_layer)\n",
    "lstm.add(SpatialDropout1D(0.2))\n",
    "lstm.add(Bidirectional(CuDNNLSTM(100)))\n",
    "lstm.add(Dense(48, activation='relu'))\n",
    "lstm.add(Dense(24, activation='relu'))\n",
    "lstm.add(Dense(12, activation='softmax'))\n",
    "\n",
    "checkpoint_filepath = 'LSTM1'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1)\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "op = keras.optimizers.Adam(learning_rate = lr_schedule)\n",
    "lstm.compile(optimizer = op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(lstm.summary())\n",
    "\n",
    "# Saves a History object for plotting\n",
    "history = lstm.fit(X_train_pad, y_train, batch_size=128, epochs=40, \n",
    "                    validation_data=(X_val_pad, y_val), verbose=2, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_fO3x_K2tQ6y",
    "outputId": "2a476479-b6d0-47c2-fc4a-d7a929ca57e4"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy as a function of training epoch\n",
    "\n",
    "figure(num=None, figsize=(5, 5), dpi=300)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.grid(True)\n",
    "plt.title('LSTM with Fasttext Embeddings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhmRn10ItUid",
    "outputId": "a8ac2d8a-12bf-41d4-acf9-4299c699a6e8"
   },
   "outputs": [],
   "source": [
    "lstm.load_weights(checkpoint_filepath)\n",
    "\n",
    "results = lstm.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DoCyNWjAvgVb",
    "outputId": "edffd9c7-4a65-4468-9ba0-29dd902746b3"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "alphabetical = []\n",
    "\n",
    "for el in mood:\n",
    "    if (el not in alphabetical):\n",
    "        alphabetical.append(el)\n",
    "\n",
    "alphabetical = sorted(alphabetical)\n",
    "\n",
    "# Gets predicted labels from model\n",
    "y_pred = lstm.predict(X_test_pad)\n",
    "\n",
    "# Formats labels into appropriate class designations\n",
    "y_pred_classes = np.zeros(len(y_pred)).astype(int)\n",
    "y_test_classes = np.zeros(len(y_test)).astype(int)\n",
    "\n",
    "for index, label in enumerate(y_pred):\n",
    "    y_pred_classes[index] = np.argmax(y_pred[index])\n",
    "    \n",
    "for index, label in enumerate(y_test):\n",
    "    y_test_classes[index] = np.argmax(y_test[index])\n",
    "\n",
    "# Generates confusion matrix\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "# Formats and displays the confusion matrix\n",
    "figure(num=None, figsize=(5, 5), dpi=300)\n",
    "df_cm = pd.DataFrame(cm, index = alphabetical, columns = alphabetical)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=plt.cm.Blues, fmt='g', cbar=False)\n",
    "plt.title('LSTM with Fasttext Embeddings Confusion Matrix', fontsize=9)\n",
    "plt.xlabel(\"Predicted Class\", fontsize=9)\n",
    "plt.ylabel(\"Actual Class\", fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymgbWshkCrUR",
    "outputId": "32db3822-ba3f-4015-ce1a-c26aa031580f"
   },
   "outputs": [],
   "source": [
    "# Cell for getting information about each class\n",
    "\n",
    "new_test_classes = np.full(shape=y_test_classes.shape[0], fill_value=\"yoooooooooooooooo\")\n",
    "new_pred_classes = np.full(shape=y_pred_classes.shape[0], fill_value=\"yoooooooooooooooo\")\n",
    "\n",
    "for x in range(len(y_test_classes)):\n",
    "    id1 = y_test_classes[x]\n",
    "    id2 = y_pred_classes[x]\n",
    "\n",
    "    word1 = alphabetical[id1]\n",
    "    word2 = alphabetical[id2]\n",
    "\n",
    "    new_test_classes[x] = word1\n",
    "    new_pred_classes[x] = word2\n",
    "\n",
    "print(classification_report(new_test_classes, new_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LY80hwWyg2L",
    "outputId": "521ee4f7-bc51-4767-d68c-34586f6bf5e1"
   },
   "outputs": [],
   "source": [
    "# Defines the Multiclass model, compiles and trains it\n",
    "\n",
    "gru = Sequential()\n",
    "embedding_layer = Embedding(num_words, \n",
    "                            EMBEDDING_DIM, \n",
    "                            embeddings_initializer=Constant(embedding_matrix), \n",
    "                            input_length=max_num_tokens,\n",
    "                            trainable=False)\n",
    "\n",
    "gru.add(embedding_layer)\n",
    "gru.add(SpatialDropout1D(0.2))\n",
    "gru.add(Bidirectional(CuDNNGRU(100)))\n",
    "gru.add(Dense(48, activation='relu'))\n",
    "gru.add(Dense(24, activation='relu'))\n",
    "gru.add(Dense(12, activation='softmax'))\n",
    "\n",
    "checkpoint_filepath = 'GRU1'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1)\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "op = keras.optimizers.Adam(learning_rate = lr_schedule)\n",
    "gru.compile(optimizer = op, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(gru.summary())\n",
    "\n",
    "# Saves a History object for plotting\n",
    "history = gru.fit(X_train_pad, y_train, batch_size=128, epochs=40, \n",
    "                    validation_data=(X_val_pad, y_val), verbose=2, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PYeUboxl3GNK",
    "outputId": "bea2e015-1957-4100-c4d2-65774981dedf"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy as a function of training epoch\n",
    "\n",
    "figure(num=None, figsize=(5, 5), dpi=300)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.grid(True)\n",
    "plt.title('GRU with Fasttext Embeddings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rc_9Blm3NBK",
    "outputId": "56172427-9da5-421a-e3d3-aa911a0ef2c0"
   },
   "outputs": [],
   "source": [
    "gru.load_weights(checkpoint_filepath)\n",
    "\n",
    "results = gru.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "print(gru.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WN0qVWF64QAg",
    "outputId": "585cf225-0043-45f1-bef1-225622930a5e"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "alphabetical = []\n",
    "\n",
    "for el in mood:\n",
    "    if (el not in alphabetical):\n",
    "        alphabetical.append(el)\n",
    "\n",
    "alphabetical = sorted(alphabetical)\n",
    "\n",
    "# Gets predicted labels from model\n",
    "y_pred = gru.predict(X_test_pad)\n",
    "\n",
    "# Formats labels into appropriate class designations\n",
    "y_pred_classes = np.zeros(len(y_pred)).astype(int)\n",
    "y_test_classes = np.zeros(len(y_test)).astype(int)\n",
    "\n",
    "for index, label in enumerate(y_pred):\n",
    "    y_pred_classes[index] = np.argmax(y_pred[index])\n",
    "    \n",
    "for index, label in enumerate(y_test):\n",
    "    y_test_classes[index] = np.argmax(y_test[index])\n",
    "\n",
    "# Generates confusion matrix\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "# Formats and displays the confusion matrix\n",
    "figure(num=None, figsize=(5, 5), dpi=300)\n",
    "df_cm = pd.DataFrame(cm, index = alphabetical, columns = alphabetical)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=plt.cm.Blues, fmt='g', cbar=False)\n",
    "plt.title('GRU with Fasttext Embeddings Confusion Matrix', fontsize=9)\n",
    "plt.xlabel(\"Predicted Class\", fontsize=9)\n",
    "plt.ylabel(\"Actual Class\", fontsize=9)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "all_GloVe.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
